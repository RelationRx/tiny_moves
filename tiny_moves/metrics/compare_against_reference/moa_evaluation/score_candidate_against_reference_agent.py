import json

from autogen import AssistantAgent

from tiny_moves.representations.structured_representations.triples import (
    ListOfEvaluationResults,
    ListOfTriples,
    assign_directionality_and_sign,
)
from tiny_moves.utils.llm_configs.llm_configs import gpt4o_list_of_evaluations

from .utils import format_triples_as_prompt

EVALUATOR_PROMPT = """
You are an expert biomedical knowledge evaluator.
Your task is to analyze a set of **reference triples** representing ground-truth biological mechanisms,
and compare them against a **candidate text** generated by a hypothesis discovery system.

Go through every triple in the reference set of statements one by one
and for every statement compare to the candidate text and complete the following tasks **in the candidate text**:

1. Assess whether the triple is present in the text?
By this we mean: does the text explicitly describe _a_ relationship
between the object and subject, even if the predicate doesn't match?
Relationships may be symmetric or asymmetric so do not focus on the whether the entities are
in the correct subject or object position relative to the reference statements.
False if there is no direct linking or either of the entities doesn't exist.
True if entitites are explicitly connected, even if the relationship is completely wrong.

Example 1:
Reference triple: ["Gene X", "activates", "Pathway Y"]
Candidate text: "Gene X is involved in the activation of Pathway Y."
Exists = 1 # entities explicitly linked

Example 2
Reference triple: ["Gene X", "upregulates", "Gene Y"]
Candidate text: "Gene X inhibits Gene Y."
Exists = 1 # entities explicitly linked, even if the relationship is wrong

Example 3
Reference triple: ["Gene X", "binds to", "Gene Y"]
Candidate text: "Gene Y binds to Gene X."
Exists = 1 # entities explicitly linked, even thought subjects and objects are swapped.


2. Assess whether the directionality of the relationship is described correctly

Is the directionality of the relationship between the entities correct? 

Correctness will depend on the identity
of the predicate and whether it is directional or not. 

In the case where the predicate is directional, ensure the subject and the object 
are correct. 

If there is no implied directionality in both the reference and
the candidate then return True.

If one is directional and the other is not, return False.

Question Criteria: Only consider this question if the existence is True.
For triples which do not meet the question criteria, return None.


3. Assess whether the sign of the relationship is described correctly
Is the sign of the relationship between the entities correct in comparison to the reference statements?
You will need to reason over the implied sign in the candidate text and compare it to the reference triple.

There may be simple cases where A inhibits B, but there may be more complex cases where the relationship
is implied by a combination of relationships or the context of the text.

For example; if the candidate text describes how inhibition A downregulates B,
but the referemce triple states that A upregulates B, this would be True.
The combination of inhibition and downregulation applied to the same relationship is equivalent to upregulation.

Question Criteria: Only consider this question if the directionality is correct.
For triples which do not meet the question criteria, return None.


4. Extract fragments of the candidate hypothesis that serve as evidence for your judgement of Q2 and Q3
Be concrete and specific.

### Formatting Rules (STRICT):

You must return your results in the ListOfEvaluationResults format.
"""


def evaluate_candidate_against_reference(
    reference_triples: ListOfTriples,
    candidate_text: str,
    seed: int,
) -> ListOfEvaluationResults:
    """
    Evaluate the candidate text against the gold standard reference triples.

    Args:
        reference_triples: The reference triples representing ground-truth biological mechanisms.
        candidate_text: The candidate text generated by a hypothesis discovery system.
        seed: Random seed for reproducibility. Defaults to 42.

    Returns:
        A ListOfEvaluationResults containing the evaluation results.

    """
    agent = AssistantAgent(
        name="evaluator",
        system_message=EVALUATOR_PROMPT,
        llm_config=gpt4o_list_of_evaluations(seed=seed),
    )
    signed_reference_triples = [
        assign_directionality_and_sign(reference_triple) for reference_triple in reference_triples.triples
    ]
    user_prompt = f"""
        Reference triples:
        \"\"\"
        {format_triples_as_prompt(signed_reference_triples)}
        \"\"\"

        Candidate text:
        \"\"\"
        {candidate_text}
        \"\"\"
        Follow the formatting rules strictly. Do not return any explanation.
        """

    reply = agent.generate_reply([{"role": "user", "content": user_prompt}])
    content = reply["content"] if isinstance(reply, dict) else reply
    parsed = json.loads(content)
    return ListOfEvaluationResults.model_validate(parsed)
